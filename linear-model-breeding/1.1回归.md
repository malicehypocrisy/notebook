# 一、线性回归

## 1.**误差平方和最小化**（求导）

* 误差平方和最小：$RRS=\sum(y_{i}-(b_{0}+b_{1}x_{i}))^{2}$,其中$b_{0}$是简单线性回归的公式的截距。

$$

R = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
$$



## 2.梯度下降

### &#9312;梯度下降法的步骤

&#9332;**初始化参数**：初始化 $w $和$b$ 的值（通常设为 0 或随机值)。

&#9333;**计算损失函数**：计算当前参数下的损失函数值 $J(w,b)$。

&#9334;**计算梯度**：计算损失函数对 $w$和 $b $的偏导数。

&#9335;**更新参数**：根据梯度更新$w$ 和 $b$。

&#9336;**重复迭代**：重复步骤 2 到 4，直到损失函数收敛或达到最大迭代次数。

### &#9313;损失函数

#### &#9332;均方误差（MSE）用作损失函数

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
$$

其中：
- $ \hat{y}_i =wx_i + b  $
- $m $：样本数量
- $\hat{y}_i $ ：模型预测值

#### &#9333; 梯度计算
梯度是损失函数对参数的偏导数，表示参数调整方向：

对 $w$ 求偏导：$\frac{\partial J}{\partial w} = -\frac{1}{m} \sum_{i=1}^{m} x_i (y_i - \hat{y}_i)$

对 $b$求偏导：$\frac{\partial J}{\partial b} = -\frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)$

#### &#9334;更新规则
通过梯度下降逐步更新参数 \( w \) 和 \( b \)：

$$
w_{\text{新}} = w_{\text{旧}} - \alpha \cdot \frac{\partial J}{\partial w}
\\
b_{\text{新}} = b_{\text{旧}} - \alpha \cdot \frac{\partial J}{\partial b}
$$


- $ \alpha$ ：学习率（步长），控制更新幅度（需手动设定，过大可能导致震荡，过小收敛慢）。
- 迭代终止条件：达到最大迭代次数，或损失函数变化小于阈值。

# 二、逻辑回归

## 1.公式

* 处理分类模型(0或1)**

$$

\log\left(\frac{p}{1-p}\right) = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_k x_k
$$

## 2.梯度下降

### &#9312;损失函数

$$
\ell(w) = \sum_{i=1}^{m} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$



# 三、多元线性回归

## 1.存在交互作用与多项式作用

$$
线性回归模型（Linear Regression）:

y = b_0 + b_1 x_1 + b_A x_A + \cdots + b_C x_C + e

\\
交互项模型（Interaction Model）:

y = b_0 + b_1 x_1 + b_A x_A + b_E (x_1 \cdot x_A) + e
\\
多项式回归模型（Polynomial Regression）
y = b_0 + b_1 x + b_A x^A + \cdots + b_C x^C + e
$$

## 2.典型相关分析（CCA）

### &#9312;分析步骤：

&#9332;数据预处理：处理缺失值，标准化变量。

&#9333;协方差矩阵计算：构建 $\mathbf{X}$ 和 $\mathbf{Y }$的协方差分块矩阵。
$$
\begin{bmatrix}
\theta \\
n
\end{bmatrix}
=
\begin{bmatrix}
V_{11} & V_{12} \\
V_{21} & V_{22}
\end{bmatrix}
$$
&#9334;求解典型相关系数：通过最大化目标函数找到最佳权重 a 和 b。
$$
p = \max_{a, b} \left\{ \frac{a^T V_{12} b}{\sqrt{a^T V_{11} a \cdot b^T V_{22} b}} \right\}
$$
&#9335;结果解释：典型相关系数 ρ反映两组变量整体关联强度。权重系数$a_i$、$b_j$揭示各变量对典型变量的贡献度。



# 四、多元回归

# 五、正则化回归

> [!NOTE]
>
> IID独立同分布

# 六、机器学习

|   特性   |             监督学习             |             非监督学习             |
| :------: | :------------------------------: | :--------------------------------: |
| 数据标签 |           需要标签数据           |            无需标签数据            |
| 学习目标 |     预测输出值（分类或回归）     |    发现数据结构（聚类、降维等）    |
| 算法类型 |    线性回归、SVM、神经网络等     |       K均值、PCA、自编码器等       |
| 数据需求 |      需要标注数据，成本较高      |   无需标注数据，适用于大规模数据   |
| 模型评估 | 明确的指标（准确率、均方误差等） | 评估较困难，依赖主观判断或间接指标 |
| 应用场景 |     预测任务（房价、分类等）     |     探索性任务（聚类、降维等）     |